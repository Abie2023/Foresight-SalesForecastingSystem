version: '3.8'

services:

    # dev workspace
    jupyter:
        # platform: linux/x86_64 # for others (mostly) or can just remove this line
        platform: linux/arm64 # for Mac M1
        container_name: jupyter_dev_repo
        shm_size: '2gb'
        build:
            context: ./services/jupyter
            dockerfile: Dockerfile
            args:
                NB_USER: ${JUPYTER_USER}
                NB_PWD: 123456789
                NB_UID: 1412
                CONDA_DIR: /opt/anaconda3
                ARCH: aarch64 # aarch64 for Mac M1 | x86_64 for others (mostly)
                JUPYTER_PORT: ${JUPYTER_PORT}
                MLFLOW_ARTIFACT_ROOT: ${MLFLOW_ARTIFACT_ROOT}
                CENTRAL_STORAGE_PATH: /home/${JUPYTER_USER}/central_storage
                MAIN_CONDA_ENV_NAME: computer-viz-dl
        env_file:
            - .env
        environment:
            - MLFLOW_TRACKING_URI=http://mlflow:${MLFLOW_PORT}
        networks:
            - forecast_network
        ports: 
            - "${JUPYTER_PORT}:${JUPYTER_PORT}"
        volumes:
            - ./:/home/${JUPYTER_USER}/workspace/
            - conda_data:/opt/anaconda3

    kafka:
        platform: linux/arm64
        container_name: kafka
        image: bitnami/kafka:latest
        ports:
            - 9094:9094
        networks:
            - forecast_network
        environment:
            - KAFKA_CFG_NODE_ID=0
            - KAFKA_CFG_PROCESS_ROLES=controller,broker
            - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
            - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094
            - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
            - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
            - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
        volumes:
            - kafka_data:/bitnami/kafka

    # mockup api producer
    data_producer:
        platform: linux/arm64
        build:
            context: ./services/data_producer
            dockerfile: Dockerfile
        env_file:
            - .env
        environment:
            - KAFKA_BOOTSTRAP_SERVER=kafka:9092
        networks:
            - forecast_network
        volumes:
            - ./services/data_producer/datasets/:/service/datasets/
        # note: without executable cmd format like this, chaining multiple commands won't work
        command: ["bash", "-c", "python scripts/put_data_in_postgres.py && python scripts/kafka_producer.py"]

    # sales forecasst service
    forecast_service:
        platform: linux/arm64
        image: ariya23156/sale-forecast-service:latest
        container_name: forecast_service
        build:
            context: ./services/forecast_service
            dockerfile: Dockerfile
            args:
                FORECAST_SERVICE_PORT: ${FORECAST_SERVICE_PORT}
        env_file:
            - .env
        environment:
            - MLFLOW_TRACKING_URI=http://mlflow:${MLFLOW_PORT}
        networks:
            - forecast_network
        volumes:
            - mlflow_data:${MLFLOW_ARTIFACT_ROOT}

    # Web UI interface
    web_ui:
        platform: linux/arm64
        container_name: web_ui
        build:
            context: ./services/web_ui
            dockerfile: Dockerfile
            args:
                WEB_UI_PORT: ${WEB_UI_PORT}
        env_file:
            - .env
        environment:
            - FORECAST_ENDPOINT_URI=http://nginx/api/forecasters/forecast
        networks:
            - forecast_network
        volumes:
            - /Users/ariyasontrapornpol/Desktop/personal_projects/dev_workspace/dev_repos/sales-forecast-mlops-at-scale/services/web_ui/app/:/service/app/

    # service for triggering training/retraining
    training_service:
        platform: linux/arm64
        container_name: training_service
        build:
            context: ./services/training_service
            dockerfile: Dockerfile
            args:
                FORECAST_SERVICE_PORT: ${TRAINING_SERVICE_PORT}
                MLFLOW_ARTIFACT_ROOT: ${MLFLOW_ARTIFACT_ROOT}
        env_file:
            - .env
        environment:
            - MLFLOW_TRACKING_URI=http://mlflow:${MLFLOW_PORT}
            - DB_CONNECTION_URL=postgresql://spark_user:SuperSecurePwdHere@postgres:${POSTGRES_PORT}/spark_pg_db
        networks:
            - forecast_network
        depends_on:
            ray:
                condition: service_healthy
        volumes:
            - ./services/training_service/app/:/service/app/
            - mlflow_data:${MLFLOW_ARTIFACT_ROOT}

    ray:
        platform: linux/arm64
        image: rayproject/ray:2.9.3-py39-cpu-aarch64
        build:
            context: ./services/ray
            dockerfile: Dockerfile
            args:
                MLFLOW_ARTIFACT_ROOT: ${MLFLOW_ARTIFACT_ROOT}
        shm_size: '2gb'
        # NOTE: if you change RAY_METRICS_EXPORT_PORT in .env, 
        # you need to change the port in prometheus.yaml too
        env_file:
            - .env
        environment:
            - RAY_GRAFANA_HOST=http://grafana:3000
            - RAY_PROMETHEUS_HOST=http://prometheus:9090
            - RAY_PROMETHEUS_NAME=Prometheus
            - RAY_GRAFANA_IFRAME_HOST=http://localhost:${GRAFANA_PORT}
        networks:
            - forecast_network
            - backend_network
        ports:
            - ${RAY_DASHBOARD_PORT}:${RAY_DASHBOARD_PORT}
        healthcheck:
            test: ["CMD-SHELL", "ray status"]
            interval: 5s
            timeout: 5s
            retries: 10
        volumes:
            - mlflow_data:${MLFLOW_ARTIFACT_ROOT}

    nginx:
        platform: linux/arm64
        container_name: nginx
        # restart: always
        build:
            context: ./services/nginx
            dockerfile: Dockerfile
        env_file:
            - .env
        networks:
            - forecast_network
        ports:
            - ${NGINX_PORT}:${NGINX_PORT}
        depends_on:
            - forecast_service
            - web_ui
            - training_service

    # ML platform / experiment tracking
    mlflow:
        platform: linux/arm64
        container_name: mlflow_server
        # restart: always
        build:
            context: ./services/mlflow
            dockerfile: Dockerfile
            args:
                MLFLOW_PORT: ${MLFLOW_PORT}
        env_file:
            - .env
        environment:
            - BACKEND_STORE_URI=postgresql://mlflow_user:SuperSecurePwdHere@postgres:${POSTGRES_PORT}/mlflow_pg_db
        networks:
            - forecast_network
        ports:
            - "${MLFLOW_PORT}:${MLFLOW_PORT}"
        volumes:
            # note: this path must be mounted/accessible for all mlflow server and clients
            - mlflow_data:${MLFLOW_ARTIFACT_ROOT}
        depends_on:
            postgres:
                condition: service_healthy

    # sql database
    postgres:
        platform: linux/arm64
        container_name: postgres_server
        image: postgres:15.3
        # restart: always
        environment:
            - POSTGRES_USER=postgres
            - POSTGRES_PASSWORD=postgres
            - POSTGRES_DB=postgres
        networks:
            - forecast_network
        volumes:
            - ./services/postgres/docker_postgres_init.sql:/docker-entrypoint-initdb.d/docker_postgres_init.sql
            - pgdata:/var/lib/postgresql/data
        healthcheck:
            test: ["CMD-SHELL", "pg_isready -U postgres"]
            interval: 5s
            timeout: 5s
            retries: 5
    
    # ui for database
    pgadmin:
        platform: linux/arm64
        image: dpage/pgadmin4
        # restart: always
        environment:
            - PGADMIN_DEFAULT_EMAIL=pgadmin@gmail.com
            - PGADMIN_DEFAULT_PASSWORD=SuperSecurePwdHere
        networks:
            - forecast_network
        ports:
            - "16543:80"
        volumes:
            - pgadmin_data:/var/lib/pgadmin
        depends_on:
            postgres:
                condition: service_healthy

    # overall monitoring & dashboards
    grafana:
        platform: linux/arm64
        image: grafana/grafana-oss:latest
        container_name: grafana
        restart: unless-stopped
        networks:
            - backend_network
        ports:
            - "${GRAFANA_PORT}:3000"
        volumes:
            - ./services/grafana/grafana_datasources.yml:/etc/grafana/provisioning/datasources/grafana_datasources.yml:ro
            - ./services/grafana/grafana_dashboards.yml:/etc/grafana/provisioning/dashboards/grafana_dashboards.yml:ro
            - ./services/grafana/dashboards:/opt/grafana/dashboards
            - ./services/grafana/grafana.ini:/etc/grafana/grafana.ini
            - grafana_data:/var/lib/grafana
        depends_on:
            - prometheus

    # time-series database
    prometheus:
        platform: linux/arm64
        image: prom/prometheus:latest
        container_name: prometheus
        restart: unless-stopped
        networks:
            - backend_network
        ports:
            - "${PROMETHEUS_PORT}:9090"
        volumes:
            - ./services/prometheus/prometheus.yaml:/etc/prometheus/prometheus.yaml:ro
            - prometheus_data:/prometheus
        command: "--config.file=/etc/prometheus/prometheus.yaml"
    
    # host machine's metrics exporter for prometheus
    node_exporter:
        platform: linux/arm64
        image: quay.io/prometheus/node-exporter:v1.5.0
        container_name: node_exporter
        restart: unless-stopped
        pid: host
        networks:
            - backend_network
        volumes: 
            - /:/host:ro,rslave
        command: "--path.rootfs=/host"

    # cadvisor
    cadvisor:
        image: gcr.io/cadvisor/cadvisor:v0.47.0   
        container_name: cadvisor
        restart: unless-stopped
        networks:
            - backend_network
        ports:
            - "${CADVISOR_PORT}:8080"
        volumes:
            - /:/rootfs:ro
            - /var/run:/var/run:ro
            - /sys:/sys:ro
            - /var/lib/docker/:/var/lib/docker:ro
            - /dev/disk/:/dev/disk:ro
            # this line is needed to make it work on Mac M1
            - /var/run/docker.sock:/var/run/docker.sock:ro
        devices:
            - /dev/kmsg
        privileged: true

networks:
    forecast_network:
        driver: "bridge"
    backend_network:
        driver: "bridge"

volumes:
    mlflow_data:
    pgdata:
    pgadmin_data:
    kafka_data:
    prometheus_data:
    grafana_data:
    # for dev
    conda_data:
    